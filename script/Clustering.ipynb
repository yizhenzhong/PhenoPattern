{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phenotyping algorithm clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to cluster the sentence fragments extracted from phenotyping algorithms with KMeans and Non-negative matrix factorization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import csv,os\n",
    "import random\n",
    "import string\n",
    "import logging\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "import nltk\n",
    "import pickle\n",
    "from numpy import linalg as LA\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_curve, auc,f1_score,adjusted_rand_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from pprint import pprint\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from optparse import OptionParser\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, adjusted_mutual_info_score, roc_auc_score, mutual_info_score, make_scorer, roc_curve,accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filepath,i):\n",
    "    name = []\n",
    "    with open(filepath, \"rb\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader, None)\n",
    "        for row in reader:\n",
    "            if len(row) == 0:\n",
    "                continue\n",
    "            name.append(row[i])\n",
    "    return name\n",
    "\n",
    "#find the row which is 1\n",
    "def attach(feature,list):\n",
    "    index = [i for i, j in enumerate(list) if j == '1']\n",
    "    text = [feature[i] for i in index] \n",
    "    return text\n",
    "\n",
    "def match_feature(whole_text,traintext,testtext,feature):\n",
    "\ttrain = []\n",
    "\ttest = []\n",
    "\t#print traintext\n",
    "\tfor i in traintext:\n",
    "\t\ttrain.append(feature[whole_text.index(i)])\n",
    "\tfor i in testtext:\n",
    "\t\ttest.append(feature[whole_text.index(i)])\n",
    "\treturn train,test\n",
    "\n",
    "def makeSiteMatrix(wholesite,site_category):\n",
    "    site_matrix = []\n",
    "    for site in wholesite:\n",
    "        row = []\n",
    "        for c in site_category:\n",
    "            if c == site:\n",
    "                row.append(1)\n",
    "            else:\n",
    "                row.append(0)\n",
    "        site_matrix.append(row)\n",
    "    return site_matrix\n",
    "\n",
    "def removeNonAscii(s): \n",
    "    \"\"\"\n",
    "    s should be a utf-8 encoded string\n",
    "    \"\"\"\n",
    "    return \"\".join(i for i in s if ord(i)<128)\n",
    "\n",
    "def token(i):\n",
    "    a =  nltk.word_tokenize(i)\n",
    "    words = []\n",
    "    for j in a:\n",
    "        words.append(stemmer.stem(j))\n",
    "    return words \n",
    "\n",
    "def token_unstem(i):\n",
    "    a =  nltk.word_tokenize(i)\n",
    "    return a\n",
    "\n",
    "\n",
    "def words( TEXT ):\n",
    "    train = [] \n",
    "    #print TEXT  \n",
    "    for si in TEXT:\n",
    "        i = removeNonAscii(si)\n",
    "        i = i.lower()\n",
    "        #words = re.sub(\"[/*'-]\", \" \",i)\n",
    "        words = re.sub(\"[/'-]\", \" \",i) #replace with space\n",
    "        words = words.replace(\"[\", \" \")\n",
    "        words = words.replace(\"]\", \" \")\n",
    "        words = words.replace(\"(\", \" \")\n",
    "        words = words.replace(\")\", \" \")   \n",
    "        words = words.replace(\"=\", \" \")   \n",
    "        words = words.replace(\">\", \" \") \n",
    "        words = words.replace(\"*\", \" \")\n",
    "        words = words.replace(\"+\", \" \")\n",
    "        words = re.sub(r\"(^|\\s)[0-9]+[.][0-9x*_.]+($|\\s)\", \" _number_ \", words) #replace icd9 codes with icd_code   \n",
    "        words = re.sub(r\"(^|\\s)[,.\\-_>=<+?*/&\\\"\\'()\\[\\]]*[0-9]+[,.\\-_>=<+?*/&\\\"\\'()\\[\\]]*($|\\s)\", \" _number_ \", words) #replace single number to _number_\n",
    "        words = re.sub(r\"(^|\\s)[0-9]+($|\\s)\", \" _number_ \", words)    #replace single number with number\n",
    "        words = re.sub(r\"\\b(one|two|three|four)\\b\",\"_number_\",words) #replace word number to _number_\n",
    "        train.append(words)\n",
    "    return train\n",
    "\n",
    "def binary(values,l,name):\n",
    "    for ss in range(len(values)):\n",
    "        if name in values[ss]:\n",
    "            l.append(1)\n",
    "        else:\n",
    "            l.append(0)\n",
    "    return l\n",
    "\n",
    "def addmatrix(tfs_train_array,matrix,features,addfeatures):\n",
    "    matrix = preprocessing.normalize(matrix, norm='l2')  \n",
    "    tfs_train = np.hstack((tfs_train_array,matrix))   \n",
    "    features = features + addfeatures \n",
    "    return tfs_train,features\n",
    "\n",
    "def bag_of_words(text):\n",
    "    tfs = countvectorizer.fit_transform(text)    \n",
    "    features_words = countvectorizer.get_feature_names()\n",
    "    tfs = preprocessing.normalize(tfs, norm='l2') \n",
    "    tfs = tfs.toarray()\n",
    "    return tfs,features_words\n",
    "\n",
    "def parse(file):\n",
    "    codelist = []\n",
    "    allcode = []\n",
    "    textlist = []\n",
    "    match = {}\n",
    "    temp = []\n",
    "    temp_stype=[]\n",
    "    temp_exp = {}\n",
    "    stype = []\n",
    "    stype_list = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            if line[0:24] == \"Processing 00000000.tx.1\":\n",
    "                temp = list(set(temp))\n",
    "                if temp != []:\n",
    "                    codelist = codelist + temp                   \n",
    "                temp = []\n",
    "                temp_stype = list(set(temp_stype))\n",
    "                # print temp_stype\n",
    "                if temp_stype != []:\n",
    "                    stype_list = stype_list + temp_stype\n",
    "                temp_stype = []\n",
    "\n",
    "                if codelist != []:\n",
    "                    allcode.append(codelist)\n",
    "                    #all_exp.append(exp_list)\n",
    "                if stype_list != []:\n",
    "                    stype.append(stype_list)\n",
    "                # print stype_list\n",
    "                text = line[26:]\n",
    "                textlist.append(text.rstrip())\n",
    "                codelist = []\n",
    "                stype_list = []\n",
    "                # print \"-----------------------------\"\n",
    "            elif line[0:6] == \"Phrase\":\n",
    "                temp = list(set(temp))\n",
    "                #temp_exp = list(set(temp_exp))\n",
    "                if temp != []:\n",
    "                    codelist = codelist + temp\n",
    "                    #z = temp_exp.copy()\n",
    "                    #z.update(codelist)\n",
    "                temp = []\n",
    "\n",
    "                temp_stype = list(set(temp_stype))\n",
    "                # print temp_stype\n",
    "                if temp_stype != []:\n",
    "                    stype_list = stype_list + temp_stype\n",
    "                temp_stype = []\n",
    "            else:\n",
    "                my = re.compile(\"C[0-9]{7}\")\n",
    "                # print line\n",
    "                result = my.findall(line)\n",
    "                if result != []:\n",
    "                    temp.append(result[0])\n",
    "                    a = line.index(\":\")\n",
    "                    temp_exp[result[0]] = str(line[a+1:-1])\n",
    "\n",
    "                my2 = re.compile(\"\\[.+\\]\")\n",
    "                result = my2.findall(line)\n",
    "                # print line\n",
    "                if result != []:\n",
    "                    temp_stype.append(result[0][1:-1])\n",
    "                    # print result[0]\n",
    "\n",
    "        codelist = codelist + list(set(temp))\n",
    "        # print temp_stype\n",
    "        stype_list = stype_list + list(set(temp_stype))\n",
    "        allcode.append(codelist)\n",
    "        stype.append(stype_list)\n",
    "    return allcode, textlist, temp_exp, stype\n",
    "\n",
    "#union of all codes\n",
    "def getunion(umls):\n",
    "    l = []\n",
    "    for i in umls:\n",
    "        l = list(set(l+i))\n",
    "    return l\n",
    "\n",
    "def umlsfeature(allcode,allwords):\n",
    "    whole = []\n",
    "    for i in allcode:\n",
    "        freq = []\n",
    "        for j in allwords:\n",
    "            if j in i:\n",
    "                freq.append(i.count(j))\n",
    "            else:\n",
    "                freq.append(0)\n",
    "        whole.append(freq)\n",
    "    whole = np.array(whole)\n",
    "    return whole\n",
    "\n",
    "def findOccurance(text, string):\n",
    "    occ = []\n",
    "    for i in text:\n",
    "        a = len([m.start() for m in re.finditer(string, i)])\n",
    "        occ.append(a)\n",
    "    return occ\n",
    "\n",
    "def reshape(x):\n",
    "    x1, x_cate = np.unique(x,return_inverse = True)\n",
    "    x_cate = x_cate.reshape(len(x),1)\n",
    "    return x_cate\n",
    "\n",
    "\n",
    "def embedding(tfs,numberOcc_train,mem):\n",
    "    tfs_train = np.dot(tfs, mem)#get train matrix \n",
    "    numberOcc_train = reshape(numberOcc_train)\n",
    "    tfs_train = np.hstack((tfs_train,numberOcc_train)) \n",
    "    tfs_train = preprocessing.normalize(tfs_train, norm='l2')\n",
    "    #get combined feature\n",
    "    features = list(range(200)) + [\"number\"]  \n",
    "    return tfs_train,features\n",
    "\n",
    "\n",
    "\n",
    "def subset(worddict,features,mem):\n",
    "    subset_embedding = []\n",
    "    for ss in features:\n",
    "        if ss in worddict:\n",
    "            subset_embedding.append(mem[worddict[ss]])\n",
    "        else:\n",
    "            print(\"word not embedded\",ss)\n",
    "            subset_embedding.append([0] * len(mem[1]))\n",
    "    subset_embedding = np.asarray(subset_embedding)\n",
    "    return subset_embedding\n",
    "\n",
    "def count_f1(count_site):\n",
    "    #TP,FP,TN,FN\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    pre = []\n",
    "    recal = []\n",
    "    for lis in count_site:\n",
    "        TP = TP + lis[0]\n",
    "        FP = FP + lis[1]\n",
    "        FN = FN + lis[3]\n",
    "        if lis[0] + lis[1] == 0:\n",
    "            pre.append(0)\n",
    "        else:\n",
    "            pre.append(lis[0]/float((lis[0]+lis[1])))\n",
    "        recal.append(lis[0]/float((lis[0]+lis[3])))\n",
    "  \n",
    "    if TP + FP == 0:\n",
    "        micro_pres = 0\n",
    "    else:\n",
    "        micro_pres = float(TP)/(TP + FP)\n",
    "    #print micro_pres\n",
    "    micro_recall = float(TP)/(TP + FN)\n",
    "    #print micro_recall\n",
    "    f1 = 2*micro_pres*micro_recall/(micro_pres + micro_recall)\n",
    "    macro_pre = sum(pre)/len(count_site) \n",
    "    macro_recall = sum(recal)/len(count_site)\n",
    "    macro_f1 = 2*macro_pre*macro_recall/(macro_pre + macro_recall)\n",
    "    return macro_f1,f1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def makeBinaryMatrix(train_umls):\n",
    "    binary_matrix = []\n",
    "    for i in range(len(train_umls)):\n",
    "        row = []\n",
    "        for j in range(len(train_umls[i])):\n",
    "            if train_umls[i][j] != 0:\n",
    "                row.append(1)\n",
    "            else:\n",
    "                row.append(0)\n",
    "        binary_matrix.append(row)\n",
    "    return binary_matrix\n",
    "    \n",
    "def readtensor(filename):\n",
    "    F = open(filename,\"r\")\n",
    "    rank = []\n",
    "    for line in F:\n",
    "        line = line.rstrip()\n",
    "        ss = line.split(\" \")\n",
    "        ss = [float(i) for i in ss]\n",
    "        rank.append(ss)\n",
    "        #rank_10 = np.asarray(rank_10)\n",
    "    return rank\n",
    "\n",
    "def printfeature(feature_matrix, feature):\n",
    "    l = feature_matrix.tolist()\n",
    "    index = [i for i, j in enumerate(l) if j != 0]\n",
    "    for j in index:\n",
    "        print(l[j], feature[j])\n",
    "    #text = [feature[i] for i in index] \n",
    "    #print text\n",
    "    return text\n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    COUNT = []\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==0 and y_hat[i] == 1:\n",
    "           FP += 1\n",
    "    \n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "    \n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==1 and y_hat[i] == 0:\n",
    "           FN += 1\n",
    "    return [TP,FP,TN,FN]\n",
    "\n",
    "def writecnninput(label,pos_out,neg_out,pos,neg):\n",
    "    for i in pos[label]:\n",
    "        pos_out.writelines(i+os.linesep)\n",
    "    for i in neg[label]:\n",
    "        neg_out.writelines(i+os.linesep)\n",
    "\n",
    "def getsubset(name,text,pattern,site,algo):\n",
    "    #######get subset index\n",
    "    index = []\n",
    "    text_sub = []\n",
    "    site_sub = []\n",
    "    pattern_sub = []\n",
    "    algo_sub = []\n",
    "    for i in range(len(pattern)):\n",
    "        if pattern[i] in name:\n",
    "            text_sub.append(text[i])\n",
    "            pattern_sub.append(pattern[i])\n",
    "            site_sub.append(site[i])\n",
    "            algo_sub.append(algo[i])\n",
    "        else:\n",
    "            #print(pattern[i])\n",
    "            continue\n",
    "    return text_sub,pattern_sub, site_sub, algo_sub\n",
    "\n",
    "\n",
    "def cluster(estimator, data, pred_label):\n",
    "    estimator.fit(data)\n",
    "    #print estimator.labels_\n",
    "    #print(\"Adjusted Rand-Index: %.3f\"\n",
    "    #  % adjusted_rand_score(pred_label, estimator.labels_))\n",
    "    return estimator.labels_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data and collect features (algorithm developing sites and algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all sentence: 190\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "pattern = []\n",
    "site = []\n",
    "algo = []\n",
    "with open(\"../data/sentence_all_info_0817.csv\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            if len(row) == 0:\n",
    "                continue\n",
    "            text.append(row[0].strip())\n",
    "            pattern.append(row[1])\n",
    "            site.append(row[2])\n",
    "            algo.append(row[3])\n",
    "            \n",
    "print(\"number of all sentence:\", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sentences to a dict, in which the key is the sentence and value is the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique sentence: 154\n"
     ]
    }
   ],
   "source": [
    "match = {}\n",
    "for i in range(len(text)):\n",
    "    if text[i] in match:\n",
    "        match[text[i]].append(pattern[i])\n",
    "      \n",
    "    else:\n",
    "        match[text[i]] = [pattern[i]]\n",
    "\n",
    "print(\"number of unique sentence:\", len(match.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only use 6 classes with large number of sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique sentence after subsetting: 131\n"
     ]
    }
   ],
   "source": [
    "#name = [\"Medication Details\",\"Confirm Disease Was Checked\",\"Rule of N\",\"Use Distinct Dates\",\"Level of Evidence\",\"Credentials of the Actor\",\"Where Did It Happen?\",\"Check For Negation\"]#\n",
    "name = [\"Confirm Disease Was Checked\",\"Rule of N\",\"Use Distinct Dates\",\"Credentials of the Actor\",\"Where Did It Happen?\",\"Check For Negation\"]#\n",
    "#name = [\"Rule of N\",\"Use Distinct Dates\"]\n",
    "text_sub,pattern_sub,site_sub,algo_sub = getsubset(name,text,pattern,site,algo)\n",
    "unique_text_sub = list(set(text_sub))\n",
    "\n",
    "\n",
    "match_sub = {}\n",
    "for i in range(len(text_sub)):\n",
    "    if text_sub[i] in match_sub:\n",
    "        match_sub[text_sub[i]].append(pattern_sub[i])\n",
    "      \n",
    "    else:\n",
    "        match_sub[text_sub[i]] = [pattern_sub[i]]\n",
    "\n",
    "    \n",
    "unique_pattern = []\n",
    "for i in match_sub.values():\n",
    "    unique_pattern.append(i[0])\n",
    "\n",
    "print(\"number of unique sentence after subsetting:\", len(match_sub.keys()))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences with unique label:  101\n"
     ]
    }
   ],
   "source": [
    "#get the sentence that has only one label\n",
    "new_match = {}\n",
    "for k, v in match_sub.items():\n",
    "    if len(v) == 1:\n",
    "        new_match[k] = v\n",
    "        \n",
    "text_unique = new_match.keys()\n",
    "train_text = words(text_unique)\n",
    "\n",
    "label_list = new_match.values()\n",
    "\n",
    "print(\"number of sentences with unique label: \", len(text_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for i in label_list:\n",
    "    labels.append(i[0])\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "label = le.transform(labels) \n",
    "print(len(set(label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process CUI and ST features\n",
    "#read umls file\n",
    "allcode_train, textlist_train,all_exp,all_st_train = parse(\"../data/umls_train_0623.txt\")\n",
    "allcode_test,textlist_test,test_exp,all_st_test = parse(\"../data/umls_test_0624.txt\")\n",
    "\n",
    "all_text_umls = textlist_train + textlist_test\n",
    "all_cui_umls = allcode_train + allcode_test\n",
    "#all_exp_umls = all_exp + test_exp\n",
    "all_st_umls = all_st_train + all_st_test\n",
    "\n",
    "allcode_train = list(map(lambda x: all_cui_umls[all_text_umls.index(x)], text_unique))\n",
    "all_st_train = list(map(lambda x: all_st_umls[all_text_umls.index(x)], text_unique))\n",
    "\n",
    "#get the union of all CUI code and st\n",
    "CUI = getunion(allcode_train)\n",
    "st = getunion(all_st_train)\n",
    "\n",
    "all_train_CUI = umlsfeature(allcode_train,CUI)\n",
    "all_train_st = umlsfeature(all_st_train,st)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process site and algo\n",
    "site_train = list(map(lambda x: site[text.index(x)], text_unique))\n",
    "algo_train = list(map(lambda x: algo[text.index(x)], text_unique))\n",
    "\n",
    "\n",
    "#process site and algorithm features\n",
    "site_category = list(set(site_train))\n",
    "algo_category = list(set(algo_train))\n",
    "\n",
    "\n",
    "all_train_site = makeSiteMatrix(site_train,site_category)\n",
    "all_train_algo = makeSiteMatrix(algo_train,algo_category)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process embedding\n",
    "import sys  \n",
    "\n",
    "#reload(sys)  \n",
    "#sys.setdefaultencoding('utf8')\n",
    "fn = '../data/embedding/embedding_mimic3_pp200_0829.pik'\n",
    "f = open(fn, 'rb')\n",
    "a = pickle.load(f, encoding='bytes')\n",
    "(mem, hwoov, hwid) = a\n",
    "\n",
    "#subset_mem = subset(embedding_word,features,mem)\n",
    "subset_mem = np.zeros((len(features), 200))\n",
    "(mem, hwoov, hwid) = a\n",
    "for i in range(len(features)):\n",
    "    try:\n",
    "        subset_mem[i,:] = mem[hwid[str.encode(features[i])],:]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "numberOcc_train = findOccurance(train_text,\"_number_\")         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1,10,100,500,550,1000,1500,2000,3000,4000]\n",
    "\n",
    "def kmeansClustering(train):\n",
    "    c = []\n",
    "    for seed in seeds:\n",
    "        kmeans = KMeans(n_clusters=6, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', \n",
    "        verbose=0, random_state=seed, copy_x=True, n_jobs=-1)\n",
    "        cluster0 = cluster(kmeans,train,label)\n",
    "        c.append(cluster0)\n",
    "    return c\n",
    "\n",
    "def nmf_clustering(train):\n",
    "    c = []\n",
    "    if np.amin(train) < 0:\n",
    "        train = train - np.amin(train)\n",
    "    #print np.amin(train)\n",
    "    train_nmf = np.transpose(train)\n",
    "    for seed in seeds:\n",
    "        model = NMF(n_components=6, init='random', random_state=seed)\n",
    "        model.fit(train_nmf) \n",
    "        nmf = model.components_ \n",
    "        nmf = np.transpose(nmf)\n",
    "        ll = [] \n",
    "        for kk in range(nmf.shape[0]):\n",
    "            l = nmf[kk,]\n",
    "            ss = l.tolist()\n",
    "            ll.append(ss.index(max(ss)))\n",
    "        c.append(ll)\n",
    "    #print c\n",
    "    return c\n",
    "\n",
    "def ami_eva(gt, predict):\n",
    "    score = []\n",
    "    for li in predict:\n",
    "        score.append(adjusted_mutual_info_score(gt, li))    \n",
    "    return max(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp training and testing\n",
    "stop = []\n",
    "f = open(\"stoplist3.txt\")\n",
    "stop = f.read().splitlines() #stop list containing have\n",
    "stemmer = PorterStemmer()\n",
    "countvectorizer = TfidfVectorizer(tokenizer=token, stop_words=stop,decode_error =\"ignore\")\n",
    "#countvectorizer = CountVectorizer(tokenizer=token, stop_words=stop,decode_error =\"ignore\",binary = True)\n",
    "\n",
    "#print text\n",
    "train, features = bag_of_words(train_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF 0.406583864016\n",
      "Kmeans 0.226991043844\n"
     ]
    }
   ],
   "source": [
    "print(\"NMF\", ami_eva(label,nmf_clustering(train)))\n",
    "print(\"Kmeans\", ami_eva(label,kmeansClustering(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF 0.144278283197\n",
      "Kmeans 0.133392331879\n"
     ]
    }
   ],
   "source": [
    "x_train_site,features_site = addmatrix(train,all_train_site,features,site_category)\n",
    "print(\"NMF\", ami_eva(label,nmf_clustering(x_train_site)))\n",
    "print(\"Kmeans\", ami_eva(label,kmeansClustering(x_train_site)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF 0.180324205561\n",
      "Kmeans 0.224686866289\n"
     ]
    }
   ],
   "source": [
    "x_train_algo,features_algo = addmatrix(train,all_train_algo,features,algo_category)\n",
    "print(\"NMF\", ami_eva(label,nmf_clustering(x_train_algo)))\n",
    "print(\"Kmeans\", ami_eva(label,kmeansClustering(x_train_algo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF 0.33562822012\n",
      "Kmeans 0.214373448414\n"
     ]
    }
   ],
   "source": [
    "x_train_CUI,features_CUI = addmatrix(train,all_train_CUI,features,CUI)\n",
    "print(\"NMF\", ami_eva(label,nmf_clustering(x_train_CUI)))\n",
    "print(\"Kmeans\", ami_eva(label,kmeansClustering(x_train_CUI)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF 0.127200036854\n",
      "Kmeans 0.244136558757\n"
     ]
    }
   ],
   "source": [
    "x_train_st,features_st = addmatrix(train,all_train_st,features,st)\n",
    "print(\"NMF\", ami_eva(label,nmf_clustering(x_train_st)))\n",
    "print(\"Kmeans\", ami_eva(label,kmeansClustering(x_train_st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF 0.124545250932\n",
      "Kmeans 0.167508760165\n"
     ]
    }
   ],
   "source": [
    "em, feature_em = embedding(train,numberOcc_train,subset_mem)\n",
    "print(\"NMF\", ami_eva(label,nmf_clustering(em)))\n",
    "print(\"Kmeans\", ami_eva(label,kmeansClustering(em)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF 0.126855255587\n",
      "Kmeans 0.133392331879\n"
     ]
    }
   ],
   "source": [
    "x_em_site,features_site_em = addmatrix(em,all_train_site,feature_em,site_category)\n",
    "print(\"NMF\", ami_eva(label,nmf_clustering(x_em_site)))\n",
    "print(\"Kmeans\", ami_eva(label,kmeansClustering(x_em_site)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF 0.188213482531\n",
      "Kmeans 0.131965253632\n"
     ]
    }
   ],
   "source": [
    "x_em_algo,features_algo_em = addmatrix(em,all_train_algo,feature_em,algo_category)\n",
    "print(\"NMF\", ami_eva(label,nmf_clustering(x_em_algo)))\n",
    "print(\"Kmeans\", ami_eva(label,kmeansClustering(x_em_algo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF 0.203821443197\n",
      "Kmeans 0.199827495258\n"
     ]
    }
   ],
   "source": [
    "x_em_CUI,features_CUI_em = addmatrix(em,all_train_CUI,feature_em,CUI)\n",
    "print(\"NMF\", ami_eva(label,nmf_clustering(x_em_CUI)))\n",
    "print(\"Kmeans\", ami_eva(label,kmeansClustering(x_em_CUI)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF 0.135492435157\n",
      "Kmeans 0.187575890893\n"
     ]
    }
   ],
   "source": [
    "x_em_st,features_st_em = addmatrix(em,all_train_st,feature_em,st)\n",
    "print(\"NMF\", ami_eva(label,nmf_clustering(x_em_st)))\n",
    "print(\"Kmeans\", ami_eva(label,kmeansClustering(x_em_st)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
